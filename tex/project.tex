%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amssymb,amsthm,amsmath}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[accepted]{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Project Proposal for CS6780}

\begin{document} 

\twocolumn[
\icmltitle{Simple Approximation Algorithms for Partitional and Hierarchical Clustering}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Alice Paul (ajp336), Calvin Wylie (cjw278), David Lingenbrink (dal299)}{}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]


\section{Introduction}
As the data we have access to grows in variety and size and becomes cheaper to store, we need methods to efficiently learn from this data. 
Partitioning data (clustering) into similar groups (clusters) is one of the key tools to learn about a set of unlabeled data such as a set of customers who have visited a website or a set of handwritten documents.
Clustering in these situations helps us learn underlying structure and classifications of our data, and it allows large data sets to be presented in a simplified and compressed view \cite{Jain}.

There are two main branches of clustering: partitional (or flat) and hierarchical. In partitional clustering we want to partition the data into disjoint sets whereas in hierarchical clustering we are interested in finding a nested tree of partitions. Often these problems are solved using heuristic algorithms which have no guarantee on the solution produced but are typically fast and easy to implement. We will look at how local search and greedy approximation algorithms, which do have some approximation guarantee on the constructed solution and are less computationally expensive than other approximation methods, compare in practice to some of the most widely used clustering algorithms both in the quality of the solution and in computation time. 

\section{Literature Review}

Clustering algorithms date back more than 50 years, and there have been many variations introduced since\cite{Jain}. However, clustering algorithms are often formulated as NP-hard problems with non-convex objective functions, which are inherently hard to solve \cite{Jain}.  Below, we will review the widely used heuristics and known approximation algorithms for both partitional and hierarchical clustering.

\subsection{Partitional Clustering}

For partitional clustering, we often specify a number $k$, representing the number of clusters, and pairwise distances between the data points, representing a measure of dissimilarity, such as Euclidean distance.  The objective is then to identify $k$ clusters so as to minimize some function of the distances. 

Arguably the most common partitional clustering method is $k$-means, in which we want to choose $k$ cluster centers to minimize the sum of squared distances from each point to its closest cluster center \cite{Jain}.  $k$-means is usually solved using Lloyd's algorithm (also referred to simply as the $k$-means algorithm), which iteratively greedily updates the centers of each cluster (assigning points to the closest center) until the solution becomes stable \cite{Jain}. Lloyd's algorithm is only guaranteed to reach a locally optimum solution and can be very sensitive to outliers and the initial points chosen \cite{Kanungo}.

Several variations on $k$-means attempt to reduce the sensitivity and worst-case behavior, such as to minimize the maximum distance from a point to its cluster center, called $k$-center clustering. Here, a simple greedy algorithm is guaranteed to produce a solution within a factor of $2$ of the optimum, as long as the given distances satisfy the triangle inequality \cite{Gonzalez}. Furthermore, this is best approximation ratio possible unless P=NP \cite{Shmoys}.  However, $k$-center does not perform well in practice compared to other methods \cite{Murphy}.

More recently, spectral based methods have become popular for clustering. These algorithms construct a graph based on the similarity matrix and compute the eigenvectors of the normalized graph Laplacian matrix \cite{Luxburg}. These eigenvectors are used to split the current cluster in half using the eigenvector associated with the second highest eigenvalue (continuing recursively until there are $k$ clusters).  Another method is to use the eigenvectors to project the data points into a lower dimensional space on which the $k$-means algorithm is run \cite{Verma}. We will base our comparisons against the algorithm introduced by \cite{Meila} which uses the latter method and performed best in practice on several datasets \cite{Verma}.

An objective function that is somewhat in between $k$-means and $k$-center is to minimize the sum of distances from each point to the median point of its cluster, called $k$-median or $k$-medoids clustering.  There are two common heuristic algorithms for this problem. The first is a local search algorithm called partition around medoids (PAM), which starts with $k$ randomly chosen data points as centers, and looks to swap one of the centers with another data point to improve the objective value.  This repeats until no more improving swaps exist \cite{ESL}. 
Another common heuristic (which we shall call $k$-medoids throughout this paper), which is more computationally efficient than PAM \cite{Park}, is a varitation on Lloyd's $k$-means algorithm.  Starting with randomly chosen centers, we assign each data point to its closest center, and then update the center within each cluster \cite{Park}. The algorithm repeats this until the solution becomes stable.

The $k$-median problem is also a fundamental problem in approximation algorithms. However, these algorithms are often not tested in practice despite their improved guarantees on the solution. The best known approximation ratio for this problem is 2.592 \cite{Wu}. This algorithm uses linear programming and is computationally intensive compared to local search and greedy algorithms.  It is known that a local search algorithm that considers swaps of size $\leq p$ is a $(3 + \frac{2}{p})$-approximation algorithm \cite{Arya}. Note that for $p=1$, this is equivalent to PAM.  As far as we know, this algorithm has not been implemented or tested on clustering data, so the relative trade-off of solution quality and computation time as $p$ increases is unknown.

Other partitional clustering methods include MeanShift and DBSCAN, which focus on finding clusters of high density \cite{Comaniciu, Ester}. We choose not to compare against density based methods since their objective differs widely from the center-based methods above. We also choose not to compare our proposed methods against Bayesian clustering methods such as Gaussian mixture models since these methods require many more parameters (representing the prior information).  Lastly, if we relax the constraint to identify exactly $k$ clusters, and instead allow the user to input ``preferences'' on the data points being clusters, a recent method called affinity propagation has been shown to perform particularly well \cite{Frey}.  However, again, because of the difference in available information, we do not to test against this algorithm.  

\subsection{Hierarchical Clustering}

In the hierarchical clustering setting, we do not specify a number of clusters. Instead, we aim to find a hierarchy of clusters so that clusters at a given level are a merging of clusters at a lower level. This presents a larger picture about the similarity between groups of data points, and can allow the user to identify a ``natural'' level of clustering \cite{ESL}. 

The most common method for hierarchical clustering is agglomerative clustering. Agglomerative clustering starts with each data point in its own cluster, and proceeds to recursively merge the two closest clusters to form the next clustering level in the hierarchy, repeating until the entire data set is a single cluster.  There are several choices on how to measure distance in order to choose which clusters to merge. Single linkage (also called nearest neighbor) defines the distance between two clusters to be the minimum pairwise distance between the data points in each cluster.  Complete linkage clustering instead defines the distance to be the largest pairwise distance. A method between these two extremes is group average linkage which looks at the average pairwise distance between clusters. This strikes a balance between the closeness and compactness of the two techniques above and gives more reasonable results in practice \cite{ESL}. Lastly, there is Ward's method which chooses the two clusters to merge so as to minimize the sum of each cluster's variance \cite{Ward}.

Approximation algorithms for hierarchical clustering have also been studied, where we aim to identify a hierarchy of clusters so as to minimize the ratio of the objective value on every level in the hierarchy of size $k$ to the optimal objective value of the flat partitional clustering problem with $k$ clusters. Note that it may not always be possible to find a solution with approximation ratio 1. The hierarchical $k$-median problem was originally introduced by Plaxton who gave a 238.88 approximation algorithm \cite{Plaxton}. This was improved by Lin et al.~\cite{Lin} who gave an incremental approximation algorithm framework. In particular, their approach relies on a black box $k$-median solver and then uses these solutions for $k=1, \ldots, n$ to greedily construct a hierarchical clustering that increases the objective value by at most a factor of $20.71$ (or $10.03$ when randomized) at each level. Using the local $p$-swap $(3+\frac{2}{p})$-approximation algorithm mentioned above as the black-box $k$-median solver performed well in experimental tests \cite{Nagarajan}. While the authors tested the variants of this algorithm against other $k$-median approximation algorithms, it has not been tested against traditional clustering algorithms or on any labeled data sets. 

Several algorithms have been built upon agglomerative clustering to incorporate non-spherical cluster shapes and categorical data such as the CURE, ROCK, and Chameleon algorithms \cite{Guha_CURE, Guha_ROCK, Karypis}, but do not relate as much to the above approximation algorithm in objective. We also choose not compare the approximation framework with Bayesian hierarchical methods for the same reasons as above. 

\section{Partitional Clustering Approximation Algorithms}

We will focus our attention on the $k$-median problem, where we are given a set $X$ and a distance metric $d(i,j)$ for all $i,j \in X$. The goal is to find a set $S \subseteq X$ such that $|S| \leq k$ so as to minimize 
\[ f(S) = \sum_{ i\in X} \min_{j \in S} d(i,j). \]

\subsection{Local Search with Multiswaps}
Consider a local search algorithm which starts with a random subset $S$ of $k$ points. At every iteration, the algorithm looks for a set $B \subseteq S$ and a set $A \subseteq X -S$ such that $|A| = |B| \leq p$ and
\[ f(S- B \cup A) \geq f(S). \]
If the algorithm finds such sets, it swaps out $B$ from $S$ and adds $A$. If no such set exists, the algorithm outputs $S$. At every iteration, for $p$ independent of $k$ and $n$, let $\mathcal{N}$ be the size of the search neighborhood. If we ensure that the algorithm improves the objective by at least a factor of $(1- \delta) = (1- \varepsilon/\mathcal{N})$, this is a polynomial-time algorithm. Furthermore, the local search algorithm outputs $S \subseteq X$ such that $|S| = k$ and
\[ f(S) \leq \frac{1}{1-\varepsilon} \left (3 + \frac{2}{p} \right ) f(O) \]
for all $O \subseteq X$. Therefore, there is a trade-off between the computation time of this algorithm and the guaranteed quality of the solution. 

\subsection{Supermodular Function Minimization}
Center-based clustering in general can be viewed as minimizing a supermodular decreasing function subject to a cardinality constraint, a problem related to maximizing a submodular function that is well-studied in approximation algorithms. The approximation algorithms in this area use local-search or greedy methods and are therefore computationally tractable on large data. In addition, using supermodular functions means the distance measure does not have to satisfy the triangle inequality, which can be useful when considering non-vector data.

Given a set $X$, a function $g:2^{X} \mapsto \mathbb{R}$ is supermodular, if for all $T \subseteq S \subseteq X$ and $j \notin S$, 
\[ g(T \cup \{j\}) - g(T) \leq g(S \cup \{j\}) - g(S). \]
Furthermore, $g$ is non-increasing if for all $T \subseteq S \subseteq X$, $g(S) \leq g(T)$. 

Now considering the $k$-median objective defined above, for completeness we define $f(\emptyset) = \max_{j \in X} f(\{j\})$. It is then elementary to check that $f$ is supermodular and non-increasing. In general, there is no known approximation algorithm for minimizing a non-increasing supermodular function subject to a cardinality constraint. However, if we incorporate the ``steepness'' of $f$, there is a known optimal approximation algorithm \cite{Sviridenko}.

As in \cite{Sviridenko}, we define the \emph{total curvature} $c$ of a non-increasing supermodular function $f$ as 
\[ c = 1 - \min_{j \in X}  \frac{ f(X) - f(X- \{j\})}{ f(\{j\}) - f(\emptyset)}.\]
Then, for every $\varepsilon >0$ and $c \in [0,1)$, there exists a local search algorithm that produces a set $S \subseteq X$ such that $|S| \leq k$ in polynomial time such that 
\[ f(S) \leq \left( 1 + \frac{c}{1-c} \cdot e^{-1} + \frac{1}{1-c} \cdot O(\varepsilon) \right) f(O) \]
for all $O \subseteq X$ such that $|O| \leq k$, with high probability \cite{Sviridenko}. Note that $c$ will vary depending on our dataset so the approximation ratio will vary. 

This algorithm first defines linear and submodular functions on $X$:
\begin{align*}
l(A) &= \sum_{j \in A} \left [ f(\{j\}) - f(\emptyset) \right], \\
g(A) & = - l(A) - f(X \setminus A).
\end{align*}
Furthermore, we let $\hat{v}_g = \max_{j \in X} g(\{j\})$, $\hat{v}_l = \max_{j \in X} |l(\{j\})|$, and $\hat{v} = \max(\hat{v}_g , \hat{v}_l)$. 

Given a set $A \subseteq X$, we consider a two-step random sampling. We first draw a value $p \in [0,1]$ using the density function $\frac{{e}^p}{e-1}$. Then, we choose $B \subseteq A$ by selecting each $x \in A$ to be in $B$ independently with probability $p$. The expected value of $g(B)$ is given by
\[  h(A) = \sum_{B \subseteq A} g(B) \cdot \int_{0}^1 \frac{e^p}{e-1} \cdot p^{|B|-1} (1-p)^{|A|-|B|} dp. \]
We then define the potential function
\[ \psi(A) = (1-e^{-1}) h(A) + l(A). \]

Now, the algorithm sets $\delta = \frac{\varepsilon}{n}$ to be the minimum step size and starts with a set $S$ of size $n-k$. Then, while there exists $a \in S$ and $b \in X \setminus S$ such that 
\[ \psi(S -a +b) \geq \psi(S) + \delta, \]
the algorithm removes $a$ from $S$ and adds $b$. If there is no swap that increases $\psi$, then the algorithm outputs $X \setminus S$. 

Note that $\psi$ requires summing an exponential number of terms. However, we can estimate $h(A)$ (and therefore $\psi(A)$) by randomly sampling $g(B)$. If $\hat{\psi}(A)$ is an estimate of $\psi(A)$ sampled from $\Omega (\varepsilon^{-2} n^4 \ln^4 n \ln M)$ samples, then 
\[ Pr[ | \hat{\psi}(A) - \psi(A) | \geq \delta ] = O(M^{-1}). \]
By setting $M$ large enough (polynomially large), the algorithm will converge in polynomial time and will return a solution satisying the above approximation ratio with high probability \cite{Sviridenko}. As far as we are aware, this algorithm has not been tested for any clustering problems. 
 
\subsection{Implementation and Results}

We implemented the local search multiswap algorithm described above for $p=1, 2, 3$ as well as the supermodular function minimization algorithm in Python, and compared against $k$-means, $k$-medoids (with a maximum iteration count of 1000) and Shi and Meila's spectral algorithm (implemented in the scikit-learn package for Python). Note that for $p=1$, the multiswap algorithm is equivalent to the well-known PAM algorithm.  In order to calculate approximation ratios, Gurobi was used to compute the global optimum for each data set.

To achieve reasonable computation times for the multiswap and supermodular function minimization algorithms, at each iteration we first consider nearby swaps before sampling for other potential swaps. For the multiswaps algorithm, we sample $O(p\cdot n)$ potential swaps on each iteration, and for the supermodular function, sample $min(100,n^2)$ swaps where each potential function is estimated by sampling 50 subsets. We also ensure that some minimum level progress is achieved for both algorithms by setting $\varepsilon = 0.1$ for the multiswaps algorithm and $\varepsilon = 10$ for the submodular function. These values were chosen by looking at the relative performance and computation time on random Gaussian data not tested below.

We also considered a hybrid heuristic algorithm (labelled $p$-swap+ in the results) where we ran $k$-medoids for 10 iterations on the result of the multiswap search algorithm.  This is motivated by the fact that, similar to Lloyd's algorithm, $k$-medoids will quickly converge to local minima.  Since $p$-swaps avoids getting stuck in local minima, the hybrid algorithm should in theory achieve good results with minimal extra computation time over the multiswap algorithm.

For each data set and each clustering method, we report objective function values, the approximation ratio, computation time, and where applicable, adjusted mutual information (AMI).  AMI is a measure which scores accuracy of predicted clusters compared to a true labeling by measuring the agreement of two assignments. The measure is normalized to reflect chance, so that a perfect labeling receives a score of 1 and an independent random labeling receives a score of 0 in expectation.  See \cite{Vinh} for further information.  The results reported are average values across 10 trials.

The data sets tested against were:
\begin{description}
\item[Gaussian] 27 synthetic $k$-center Gaussian data sets, generated in Python. In particular, Gauss\textunderscore d\textunderscore k\textunderscore e was generated by first choosing k random centers in $[-1,1]^d$. Around each center, we then generated $(1000/k) \cdot \mathrm{Unif}[0,1]$ Gaussian points of which $e$ percent had standard deviation 0.25 (noisy) and $1-e$ percent had standard deviation $0.05$. 
\item[OR $p$-median Library] From a collection of test data sets created by \cite{Beasley} with 40 test sets of size $100, 200, \ldots, 900$ and varying values of $k$. 
\item[UCI Iris] \cite{Iris} Contains 4 attributes on 150 instances of flowers, each of with belongs to one of three species of iris.  One of the species is linearly separable from the others, but the other two are not.  One of the most used data sets in pattern recognition.
\item[UCI Soybean (Small)] \cite{Soybean} 47 instances, with 35 attributes data set of soybean disease from UCI.
\end{description} 

\begin{figure}[h]
\begin{tabular}{ | l | l | l | l | l | }
\hline
 Dataset Name & Opt Obj & $k$ & $n$ & dim \\ \hline
Soybean & 114 & 4 & 47 & 35 \\ 
Iris & 98 & 3 & 150 & 4 \\ 
Gauss\_5\_5\_5 & 195.4 & 5 & 388 & 5 \\ 
Gauss\_5\_10\_5 & 270 & 10 & 544 & 5 \\ 
Gaussian avg & 241 & 22 & 507 & 5 \\ 
pmed5 & 1355 & 33 & 100 & \  \\ 
pmed20 & 1789 & 133 & 400 & \  \\ 
% pmed40 & 5128 & 90 & 900 & \  \\ 
pmedian avg & 5535 & 46 & 460 & \  \\ \hline
 \end{tabular}
\caption{Some specifics about our datasets}
\end{figure}

Tests were conducted using Python 2.7.9 on a 64-bit Windows platform, on a 4 core 3.4 Ghz Intel i7 processor with 16 GB physical memory. Results are given in Figure 2  below with some results averaged due to space constraints. 

\begin{figure*}[h]
\label{fig:flat_results}
\small
\begin{tabular}{ | l | l | l | l | l | l | l | l | l | l | }
\hline
	Dataset Name & Time & Time & Time & Time & Time & Time & Time & Time & Time \\ \hline
	&K-means & Spectral & k-medoids & Supermodular & PAM & 1-Swap+ & 2-Swap & 2-Swaps+ & 3-Swap  \\ \hline
	Soybean & 0.015 & 0.061 & 0.002 & 1.179 & 0.026 & 0.016 & 0.048 & 0.049 & 0.081 \\ 
	Iris & 0.013 & 0.023 & 0.003 & 9.788 & 0.125 & 0.102 & 0.212 & 0.229 & 0.414 \\ 
	Gauss\_5\_5\_5 & 0.025 & 0.056 & 0.032 & 75.262 & 1.159 & 1.506 & 2.188 & 2.382 & 3.71 \\ 
	Gauss\_5\_10\_5 & 0.054 & 0.095 & 0.109 & 134.137 & 6.834 & 5.407 & 9.243 & 9.428 & 15.96 \\ 
	%Gauss\_5\_50\_5 & 0.137 & 0.196 & 0.302 & 97.336 & 27.856 & 21.446 & 32.407 & 39.614 & 44.69 \\ 
	Gaussian avg & 0.073 & 0.128 & 0.182 & 144.455 & 13.289 & 13.021 & 18.749 & 18.696 & 24.592 \\ 
	pmed5 & 0.038 & 8.298 & 0.481 & 0.622 & 0.653 & 0.748 & 0.939 & \  & \  \\ 
	pmed20 & 1.795 & 174.951 & 52.208 & 44.979 & 68.554 & 68.231 & 79.288 & \  & \  \\ 
	% pmed40 & 3.527 & 849.852 & 251.259 & 264.867 & 250.412 & 391.738 & 350.235 & \  & \  \\ 
	pmedian avg & 0.807 & 203.308 & 40.386 & 40.347 & 48.264 & 56.76 & 56.19 & \  & \  \\ \hline
\end{tabular}
\vspace{0.2 cm}

\begin{tabular}{ | l | l | l | l | l | l | l | l | l | l | l | }
\hline
	Dataset Name & AMI & AMI & AMI & AMI & AMI & AMI & AMI & AMI & AMI  \\ \hline
	& K-means & Spectral & k-medoids & Supermodular & PAM & 1-Swap+ & 2-Swap & 2-Swaps+ & 3-Swap   \\ \hline
	Soybean & 0.686 & 0.178 & 0.55 & 0.612 & 0.708 & 0.732 & 0.717 & 0.708 & 0.712 \\ 
	Iris & 0.748 & 0.757 & 0.69 & 0.7 & 0.772 & 0.760 & 0.763 & 0.757 & 0.761 \\ 
	Gauss\_5\_5\_5 & 0.741 & 0.713 & 0.687 & 0.657 & 0.742 & 0.749 & 0.741 & 0.744 & 0.737  \\ 
	Gauss\_5\_10\_5 & 0.783 & 0.736 & 0.677 & 0.575 & 0.729 & 0.725 & 0.737 & 0.737& 0.729  \\ 
	%Gauss\_5\_50\_5 & 0.646 & 0.596 & 0.621 & 0.570 & 0.622 & 0.619 & 0.627 & 0.631 & 0.625  \\ 
	Gaussian avg & 0.777 & 0.744 & 9.112 & 0.623 & 0.76 & 0.761 & 0.762 & 0.761 & 0.76 \\ \hline
\end{tabular}
\vspace{0.2 cm}

\begin{tabular}{ | l | l | l | l | l | l | l | l | l | }
\hline
	Dataset Name & Approx & Approx & Approx & Approx & Approx & Approx & Approx  \\ \hline
	& k-medoids & Supermodular & PAM & 1-Swap+ & 2-Swap & 2-Swaps+ & 3-Swap   \\ \hline
	Soybean & 1.167 & 1.199 & 1.027 & 1.012 & 1.022 & 1.012 & 1.024 \\ 
	Iris & 1.081 & 1.375 & 1.016 & 1.004 & 1.013 & 1.004 & 1.009 \\ 
	Gauss\_5\_5\_5 & 1.070 & 1.436 & 1.030 & 1.002 & 1.027 & 1.005 & 1.026  \\ 
	Gauss\_5\_10\_5 & 1.080 & 1.389 & 1.017 & 1.011 & 1.018 & 1.009 & 1.017  \\ 
	%Gauss\_5\_50\_5 & 1.0887 & 1.272 & 1.030 & 1.020 & 1.027 & 1.014 & 1.033  \\ 
	Gaussian avg & 1.064 & 1.416 & 1.025 & 1.009 & 1.023 & 1.008 & 1.024 \\ 
	pmed5 & 1.373 & 1.681 & 1.06 & 1.026 & 1.071 & 1.033 & 1.051 \\ 
	pmed20 & 1.501 & 1.73 & 1.079 & 1.032 & 1.076 & 1.027 & 1.073 \\ 
	%pmed40 & 1.187 & 1.423 & 1.046 & 1.017 & 1.043 & 1.019 & 1.039 \\ 
	pmedian avg & 1.203 & 1.456 & 1.041 & 1.017 & 1.039 & 1.016 & 1.037 \\ \hline
\end{tabular}


\caption{Average Computation Time (sec), AMI measure, and Approximation Coeffiecients for Partitional Clustering Algorithms}
\end{figure*}


In terms of computation time, our proposed algorithms were much slower than $k$-means, the spectral algorithm, or $k$-medoids. In general, the multiswaps algorithm was around two orders of magnitude slower with a small multiplicative increase as $p$ increased, and the supermodular algorithm was around four orders of magnitude slower, likely due to the extra potential function sampling the supermodular algorithm must perform. 

In addition, the supermodular function algorithm did not perform as well in terms of AMI or approximation ratio compared to the other methods. Calculating the curvature, we saw that every  test set's curvature was very close to 1, so the guarantee of the algorithm was quite poor. Despite this, the algorithm did maintain an approximation ratio below 2 for all instances and an AMI measure above 0.5 for almost all instances, averaging around 0.85 for the 7 dimensional Gaussian data. 

When increasing $p$ for the multiswaps algorithm, we did observe slight increases in computation time, but the times remained on the same order of magnitude. The increase in $p$ did not seem to have any significant effect on the AMI measure nor approximation ratio. This may be due to the fact the algorithm for $p=1$ performed quite well already with an approximation ratio very close to 1. In addition, PAM had the highest AMI measures for the iris and soybean data sets. The AMI measures for the Gaussian data set were higher than those of $k$-medoids in general but equivalent or only slightly weaker than those of the spectral method or $k$-means. The heuristic method of adding in a few rounds of $k$-medoids at the end of our multiswaps algorithm did not have much effect on the AMI measure but did boost the approximation ratio, sometimes finding an optimal solution. 

It is interesting to note that, for the Gaussian data, an increase in dimension, reduction in $k$, or reduction in the percentage of noisy data made it easier for the algorithms to classify the data set, which makes sense based on our intuition of clustering. For the OR library instances, we saw that the approximation ratio tended to increase as $k$ grew larger but not as $n$ grew. 

Overall, the results of our experiments do not suggest that the proposed approximation algorithms provide any advantage over current well known methods. However, the results do suggest that we can use sampling to run PAM with lower computational cost while still maintaining good performance accuracy in terms of AMI and approximation ratio. 

\section{Hierarchical Clustering Approximation Algorithms}
Turning now to the hierarchical clustering setting, given a set $X$ and a distance metric $d(i,j)$ for all $i,j \in X$, we will represent a solution to the $k$-median problem as a pair $(S, a)$ where $S \subseteq X$ such that $|S| \leq k$ and $a$ is an assignment function that maps each $i \in X$ to a center in $S$. Therefore, for any $i \in S$, $a^{-1}(i)$ will be all points in the cluster centered at $i$. We define $\mathrm{cost}(S,a) := \sum_{i \in X} d(i, a(i))$.

Given solution pairs $(S_1, a_1)$ and $(S_2, a_2)$, we say that $(S_1, a_1)$ is \emph{nested} in $(S_2, a_2)$ if the following conditions hold.
\begin{enumerate}
	\item $S_1 \subset S_2$, 
	\item $\forall  j \in X$, if $a_2(j) \in S_1$, then $a_1(j) = a_2(j)$, 
	\item $\forall j, k \in X$, if $a_2(j) = a_2(k)$, then $a_1(j) = a_1(k)$.
\end{enumerate}
 A solution to the hierarchical clustering problem is therefore a chain of nested solutions. However, we want this solution to stay close to the optimal $k$-median solutions when looking at any level of the hierarchy.  
Consider any $k$-median algorithm $\mathcal{A}$. After running $\mathcal{A}$ for all potential values of $k$, Lin et al.~\cite{Lin} introduce an augmentation step that will nest these clusterings into the hierarchical framework and only increase costs by a bounded factor. 

First, set $i=1$, $S_0 = \emptyset$, $\beta = 3+\sqrt{3}$, and $\beta_0 = 1$ (for the randomized algorithm we switch $\beta = 6.355$ and set $\beta_0 = \beta^X$, where $X$ is chosen from a uniform distribution on $[0,1]$). Let $V_i$ be the solutions generated from running $\mathcal{A}$ for $k=n-i+1$ for $i=1, \ldots, n$. The cost of these solutions is determined by assigning each point to the nearest cluster center and is scaled so that the minimum cost for $k<n$ is at least one. We then order these solutions according to their cost into buckets of the form $[0,0], (\beta_0, \beta_0 \beta], (\beta_0 \beta, \beta_0 \beta^2], \ldots $. 

From each non-empty bucket, pick the solution with the smallest number of cluster centers, and let these solutions be $\overline{V}_1, \overline{V}_2, \ldots \overline{V}_r$. We then inductively build our nested solutions. Given $(S_{i-1}, a_{i-1})$ and $\overline{V}_i$, we define $S_i$ by iteratively adding the closest point $j \in S_{i-1}$ to $k$ for all $k \in \overline{V}_i$. Note that $|S_{i}| \leq |\overline{V}_i|$. 

We then define an assignment $a_i$. For each $j \in X$ such that $a_{i-1}(j) \in S_{i}$, we set $a_i(j) = a_{i-1}(j)$. Otherwise, we set $a_i(j)$ to be the facility $i \in S_i$ such that assigning all points in $a_{i-1}^{-1}(j)$ (i.e. all points in $j$'s cluster in $S_{i-1}$) would have minimum cost. Using this assignment, $(a_i, S_i)$ is nested in $(a_{i-1}, S_{i-1})$. 

We continue this nesting until $|S_i| = 1$, reaching the top level of the hierarchy. After forming this hierarchical clustering, for any $k \in 1, \ldots, n$ \cite{Lin} guarantee that the cost of $S_i$ is at most $20.71$ times the cost of $V_k$ where $|S_{i-1}| \geq n-k+1 \geq |S_{i}|$ (in the randomized case the expected cost of $S_i$ is at most $10.03$ times the cost of $V_k$).

\subsection{Implementation and Results}
We implemented the nesting hierarchical framework above in Python, offering options for both the randomized and deterministic versions. As a black-box $k$-median solver, we used $k$-medoids and the 1-swap+ algorithms, discussed in the partitional clustering section above. We compared the results to hierarchical agglomerative clustering with both average linkage and Ward's linkage (implemented in the scipy package in Python). 

We also tested a more computationally friendly heuristic of the hierarchical framework which only buckets solutions for values of $k$ that are a power of two. This yields many fewer $k$-median instances to solve, reducing the run-time significantly. This was motivated by the fact that the difference between various $k$-median solutions will be larger when $k$ is small so we expect our sampling to still represent much of the underlying structure. 
 
For each data set discussed previously, and each clustering method, we report the approximation ratio, computation time, and FScore measure.  FScore is a measure that looks at the clusters represented on each level and how they represent the true clustering in terms of precision and recall. See \cite{Larsen} for more information.  Results are in Figure 3.

\begin{figure*}[h]
\label{fig:hier_results}
\small
\begin{tabular}{ | l | l | l | l | l | l | l | }
\hline
	   & Time & Time & Time & Time & Time & Time \\ \hline
	Dataset & Ave Linkage & Ward Linkage & K-med & K-med logn ran & k-med ran & PAM logn ran \\ \hline
	Soybean & 0.0004 & 0.0002 & 0.286 & 0 & 0.294 & 0.167 \\ 
	Iris & 0.0021 & 0.0011 & 3.161 & 0.226 & 3.794 & 5.122 \\ 
	Gauss\_5\_5\_5 & 0.0097 & 0.0109 & 106.619 & 2.735 &120.893 & 147.137 \\
	Gauss\_5\_10\_5 & 0.0237 & 0.0256 & 383.854 & 10.259 & 383.298 & 494.554 \\
	%Gauss\_5\_50\_5 &  & & & & & \\
	Gaussian Average & 0.0229 & 0.0239 & 354.459 & 7.492 & 369.967 & 423.293 \\ 
	pmed5 &  0.0004 & N/A & 1.184 & 0.102 & 1.288 & 1.673   \\ 
	pmed25 & 0.0174 & N/A & 279.334 & 5.011 & 306.684 & 312.503   \\ 
	pmedian avg & 0.0067 & N/A &  87.355 & 1.957 & 94.548 & 101.576   \\ \hline
\end{tabular}
\vspace{0.2 cm}

\begin{tabular}{ | l | l | l | l | l | l | l | }
\hline
	   & F-Score & F-Score & F-Score & F-Score & F-Score & F-Score \\ \hline
	Dataset & Ave Linkage & Ward Linkage & K-med & K-med logn rand & K-med rand & PAM logn rand \\ \hline
	Soybean & 0.615 & 0.554 & 0.772 & 0.643 & 0.742 & 0.633 \\ 
	Iris & 0.5 & 0.5 & 0.888 & 0.767 & 0.762 & 0.732 \\ 
	Gauss\_5\_5\_5 & 0.519 & 0.520 & 0.591 & 0.816 & 0.761 & 0.785 \\
	Gauss\_5\_10\_5 & 0.298 & 0.281 & 0.589 & 0.584 & 0.576 & 0.446 \\
	%Gauss\_5\_50\_5 &  & & & & & \\
	Gaussian Average & 0.393 & 0.378 & 0.649 & 0.616 & 0.624 & 0.624 \\ \hline
\end{tabular}
\vspace{0.2 cm}

\begin{tabular}{ | l | l | l | l | l | }
\hline
	   & Approx & Approx & Approx & Approx \\ \hline
	Dataset & K-med & K-med logn rand & K-med rand & PAM logn rand \\ \hline
	Soybean & 1.585 & 2.334 & 1.711 & 2.001 \\ 
	Iris & 2560631.459 & 2550279.566 & 2787312.009 & 1623132.096 \\ 
	Gauss\_5\_5\_5 & 6.459 & 13.394 & 8.624 & 7.791 \\
	Gauss\_5\_10\_5 &  3.420 & 3.756 & 4.196 & 2.309  \\
	%Gauss\_5\_50\_5 &  & & &  \\
	Gaussian Average & 5.182 & 7.262  & 5.537 & 4.739 \\ 
	pmed5 & 11.367 & 51.141 & 16.261 & 11.950 \\ 
	pmed25 & 6.641 & 13.372 & 5.378 & 13.200 \\ 
	pmedian avg & 10.330 & 19.822 & 10.524 & 8.599 \\ \hline
\end{tabular}

\caption{Average Computation Time (sec), FScore measure, and Approximation Coeffiecients for Hierarchical Clustering Algorithms. K-med and PAM represent the black-box $k$-median solvers used and rand and logn indicate whether the randomized and logarithmic setting were used, respectively.}
\end{figure*}

In general, the hierarchical framework performed significantly better than both methods of agglomerative clustering in terms of FScore measure, even for our logarithmic version. The FScore measures were very low for the agglomerative clustering methods, sometimes around 0.25, but were much closer to the AMI measures we saw in flat clustering for our implemented algorithms. 

For $k$-medoids in the non-logarithmic version and PAM in the logarithmic version, the hierarchical framework increased the computation time by three orders of magnitude, and increased by two orders of magnitude for $k$-medoids in the logarithmic version. Interestingly, the approximation factors did not change significantly between the randomized and deterministic versions for $k$-medoids. The approximation ratios were strongest as $n$ increased or as the dimension increased. In particular, for the Gaussian data in 7 dimensions, the approximation ratio was often around 3.  The high approximation ratio for the iris data set was due to the fact that the optimal solution for $k = n-1, n-2$ was very close to zero, and our heuristic $k$-median algorithm only sometimes picked up on these solutions. The approximation factor at lower levels was more consistent with the other results. 


\section{Conclusion and Future Work}
We implemented various simple approximation algorithms for partitional clustering, and a framework for hierarchical clustering.  We compared these algorithms, along with heuristic modifications, against the most common clustering algorithms used in practice.

For partitional clustering, we found that the approximation algorithms in the literature are in general slower and cannot improve much on current heuristics used despite their approximation guarantees. This was not true in the hierarchical clustering case where we saw significant improvement in FScore measures for our implemented methods and overall good approximation ratios where none were had before. In particular, our logarithmic nesting algorithm seems to provide the best balance between performance and computational efficiency.

In the future, we hope to explore how linear programming based methods perform in both the flat and hierarchical context as well and to implement our own agglomerative clustering to help make smarter local merges in the nesting framework.

\section{Acknowledgements}
We would like to acknowledge David P. Williamson and Karthik Sridharan for their valuable feedback on this project.

\label{submission}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{projectBib}
\bibliographystyle{icml2014}

%\appendix
%
%\section{Appendix 1: Full Results}
%
%\subsection{Gaussian Data}
%\begin{figure*}[h]
%\begin{center}
%\begin{tabular}{ | l | l | l | l | l | }
%\hline
%	Dataset & NameOpt Obj & k & n & dim  \\ \hline
%	Gauss\_3\_5\_0 & 195 & 5 & 542 & 3 \\ 
%	Gauss\_3\_5\_5 & 225 & 5 & 620 & 3 \\ 
%	Gauss\_3\_5\_10 & 158 & 5 & 441 & 3 \\
%	Gauss\_3\_10\_0 & 147 & 10 & 472 & 3 \\
%	Gauss\_3\_10\_5 & 142 & 10 & 480 & 3 \\ 
%	Gauss\_3\_10\_10 & 201 & 10 & 557 & 3 \\ 
%	Gauss\_3\_50\_0 & 108 & 50 & 471 & 3 \\
%	Gauss\_3\_50\_5 & 94 & 50 & 422 & 3 \\ 
%	Gauss\_3\_50\_10 & 95 & 50 & 408 & 3 \\ 
%	Gauss\_5\_5\_0 & 352 & 5 & 720 & 5 \\ 
%	Gauss\_5\_5\_5 & 195 & 5 & 388 & 5 \\ 
%	Gauss\_5\_5\_10 & 214 & 5 & 394 & 5 \\ 
%	Gauss\_5\_10\_0 & 346 & 10 & 703 & 5 \\ 
%	Gauss\_5\_10\_5 & 270 & 10 & 544 & 5 \\ 
%	Gauss\_5\_10\_10 & 292 & 10 & 572 & 5 \\ 
%	Gauss\_5\_50\_0 & 188 & 50 & 445 & 5 \\ 
%	Gauss\_5\_50\_5 & 178 & 50 & 408 & 5 \\ 
%	Gauss\_5\_50\_10 & 229 & 50 & 480 & 5 \\ 
%	Gauss\_7\_5\_0 & 349 & 5 & 567 & 7 \\ 
%	Gauss\_7\_5\_5 & 368 & 5 & 579 & 7 \\ 
%	Gauss\_7\_5\_10 & 356 & 5 & 527 & 7 \\ 
%	Gauss\_7\_10\_0 & 351 & 10 & 573 & 7 \\ 
%	Gauss\_7\_10\_5 & 302 & 10 & 478 & 7 \\ 
%	Gauss\_7\_10\_10 & 354 & 10 & 534 & 7 \\ 
%	Gauss\_7\_50\_0 & 257 & 50 & 451 & 7 \\ 
%	Gauss\_7\_50\_5 & 256 & 50 & 430 & 7 \\ 
%	Gauss\_7\_50\_10 & 298 & 50 & 480 & 7 \\ \hline
%\end{tabular}
%\end{center}
%
%\caption{Details For Gaussian Datasets}
%\end{figure*}
%
%\begin{figure*}[h]
%\begin{center}
%\begin{tabular}{ | l | l | l | l | l | l | l | l | l | l | l | }
%\hline
%	Dataset &Time & Time & Time & Time & Time & Time & Time & Time & Time & Time   \\ \hline
%	&K-means & Spectral & k-medoids & Supermodular & PAM & 1-Swap+ & 2-Swap & 2-Swaps+ & 3-Swap & 4-swaps   \\ 
%	Gauss\_3\_5\_0 & 0.024 & 0.056 & 0.066 & 131.305 & 2.707 & 3.195 & 4.779 & 4.138 & 7.683 & 11.325 \\ 
%	Gauss\_3\_5\_5 & 0.032 & 0.076 & 0.099 & 195.013 & 3.682 & 3.764 & 6.639 & 5.32 & 9.627 & 14.575 \\ 
%	Gauss\_3\_5\_10 & 0.033 & 0.055 & 0.047 & 80.439 & 2.244 & 1.802 & 2.806 & 3.024 & 5.107 & 7.457 \\ 
%	Gauss\_3\_10\_0 & 0.041 & 0.083 & 0.095 & 121.551 & 5.133 & 3.576 & 7.452 & 6.3 & 10.157 & 11.872 \\ 
%	Gauss\_3\_10\_5 & 0.048 & 0.1 & 0.117 & 101.072 & 5.026 & 4.96 & 8.57 & 6.952 & 11.948 & 12.225 \\ 
%	Gauss\_3\_10\_10 & 0.05 & 0.106 & 0.153 & 149.352 & 7.101 & 6.456 & 11.273 & 10.646 & 18.21 & 17.414 \\ 
%	Gauss\_3\_50\_0 & 0.154 & 0.324 & 0.435 & 175.827 & 29.201 & 34.228 & 43.65 & 49.964 & 48.259 & 66.413 \\ 
%	Gauss\_3\_50\_5 & 0.151 & 0.253 & 0.376 & 127.64 & 26.318 & 21.541 & 38.004 & 33.213 & 45.862 & 57.262 \\ 
%	Gauss\_3\_50\_10 & 0.153 & 0.265 & 0.301 & 132.119 & 21.341 & 22.556 & 35.389 & 32.139 & 44.361 & 50.143 \\ 
%	Gauss\_5\_5\_0 & 0.026 & 0.079 & 0.08 & 230.345 & 4.997 & 4.21 & 9.428 & 8.063 & 12.811 & 19.337 \\ 
%	Gauss\_5\_5\_5 & 0.025 & 0.056 & 0.032 & 75.262 & 1.159 & 1.506 & 2.188 & 2.382 & 3.71 & 5.797 \\ 
%	Gauss\_5\_5\_10 & 0.025 & 0.042 & 0.027 & 69.349 & 1.383 & 1.458 & 2.403 & 2.575 & 3.811 & 5.748 \\ 
%	Gauss\_5\_10\_0 & 0.044 & 0.1 & 0.18 & 227.747 & 11.633 & 12.129 & 17.724 & 18.654 & 26.563 & 33.646 \\ 
%	Gauss\_5\_10\_5 & 0.054 & 0.095 & 0.109 & 134.137 & 6.834 & 5.407 & 9.243 & 9.428 & 15.96 & 21.795 \\ 
%	Gauss\_5\_10\_10 & 0.045 & 0.092 & 0.117 & 127.614 & 7.639 & 6.456 & 11.807 & 12.186 & 18.339 & 23.554 \\ 
%	Gauss\_5\_50\_0 & 0.142 & 0.211 & 0.389 & 139.818 & 30.155 & 27.431 & 37.251 & 43.421 & 50.192 & 68.329 \\ 
%	Gauss\_5\_50\_5 & 0.137 & 0.196 & 0.302 & 97.336 & 27.856 & 21.446 & 32.407 & 39.614 & 44.69 & 53.546 \\ 
%	Gauss\_5\_50\_10 & 0.149 & 0.219 & 0.417 & 170.431 & 37.132 & 35.119 & 45.341 & 47.002 & 56.683 & 69.712 \\ 
%	Gauss\_7\_5\_0 & 0.019 & 0.054 & 0.052 & 185.865 & 2.975 & 3.565 & 4.954 & 5.005 & 8.531 & 11.867 \\ 
%	Gauss\_7\_5\_5 & 0.031 & 0.078 & 0.047 & 154.1 & 2.603 & 3.04 & 5.45 & 5.738 & 8.056 & 12.505 \\ 
%	Gauss\_7\_5\_10 & 0.018 & 0.052 & 0.043 & 130.185 & 2.949 & 2.478 & 4.668 & 4.849 & 7.324 & 11.036 \\ 
%	Gauss\_7\_10\_0 & 0.045 & 0.071 & 0.097 & 185.521 & 8.015 & 7.338 & 12.495 & 10.474 & 16.174 & 20.671 \\ 
%	Gauss\_7\_10\_5 & 0.04 & 0.063 & 0.073 & 136.189 & 5.315 & 5.365 & 9.287 & 6.87 & 11.768 & 16.072 \\ 
%	Gauss\_7\_10\_10 & 0.048 & 0.09 & 0.1 & 168.962 & 7.047 & 6.538 & 9.783 & 9.322 & 13.581 & 19.41 \\ 
%	Gauss\_7\_50\_0 & 0.143 & 0.199 & 0.401 & 152.958 & 32.592 & 37.747 & 39.026 & 42.999 & 60.483 & 69.667 \\ 
%	Gauss\_7\_50\_5 & 0.147 & 0.212 & 0.327 & 136.496 & 27.8 & 26.288 & 38.683 & 34.335 & 47.779 & 59.414 \\ 
%	Gauss\_7\_50\_10 & 0.149 & 0.231 & 0.426 & 163.651 & 37.968 & 41.976 & 55.514 & 50.175 & 56.302 & 79.819 \\ \hline
%\end{tabular}
%
%\end{center}
%
%\caption{Running Times For Gaussian Datasets}
%\end{figure*}
%
%\begin{figure*}[h]
%\begin{center}
%\begin{tabular}{ | l | l | l | l | l | l | l | l | l | l | l | }
%\hline
%	Dataset&AMI & AMI & AMI & AMI & AMI & AMI & AMI & AMI & AMI & AMI  \\ \hline
%	& K-means & Spectral & k-medoids & Supermodular & PAM & 1-Swap + & 2-Swap & 2-Swaps + & 3-Swap & 4-swaps   \\ \hline
%	Gauss\_3\_5\_0 & 0.884 & 0.887 & 0.729 & 0.678 & 0.88 & 0.876 & 0.884 & 0.876 & 0.881 & 0.882 \\ 
%	Gauss\_3\_5\_5 & 0.7 & 0.62 & 0.637 & 0.545 & 0.683 & 0.68 & 0.685 & 0.69 & 0.682 & 0.686 \\ 
%	Gauss\_3\_5\_10 & 0.68 & 0.618 & 0.62 & 0.504 & 0.663 & 0.665 & 0.659 & 0.659 & 0.664 & 0.668 \\ 
%	Gauss\_3\_10\_0 & 0.693 & 0.641 & 0.699 & 0.619 & 0.692 & 0.692 & 0.688 & 0.696 & 0.685 & 0.685 \\ 
%	Gauss\_3\_10\_5 & 0.534 & 0.466 & 0.516 & 0.446 & 0.534 & 0.533 & 0.535 & 0.532 & 0.531 & 0.536 \\ 
%	Gauss\_3\_10\_10 & 0.669 & 0.56 & 0.616 & 0.516 & 0.651 & 0.653 & 0.652 & 0.657 & 0.657 & 0.653 \\ 
%	Gauss\_3\_50\_0 & 0.504 & 0.498 & 0.498 & 0.475 & 0.501 & 0.497 & 0.491 & 0.497 & 0.495 & 0.501 \\ 
%	Gauss\_3\_50\_5 & 0.407 & 0.412 & 0.402 & 0.396 & 0.392 & 0.385 & 0.389 & 0.386 & 0.387 & 0.385 \\ 
%	Gauss\_3\_50\_10 & 0.421 & 0.424 & 0.406 & 0.39 & 0.413 & 0.414 & 0.408 & 0.404 & 0.405 & 0.41 \\ 
%	Gauss\_5\_5\_0 & 0.99 & 0.99 & 0.858 & 0.719 & 0.988 & 0.994 & 0.989 & 0.994 & 0.989 & 0.985 \\ 
%	Gauss\_5\_5\_5 & 0.741 & 0.713 & 0.687 & 0.657 & 0.742 & 0.749 & 0.741 & 0.744 & 0.737 & 0.747 \\ 
%	Gauss\_5\_5\_10 & 0.833 & 0.83 & 227.742 & 0.663 & 0.76 & 0.779 & 0.806 & 0.747 & 0.789 & 0.77 \\ 
%	Gauss\_5\_10\_0 & 0.974 & 0.972 & 0.858 & 0.732 & 0.96 & 0.964 & 0.964 & 0.965 & 0.96 & 0.964 \\ 
%	Gauss\_5\_10\_5 & 0.783 & 0.736 & 0.677 & 0.575 & 0.729 & 0.725 & 0.737 & 0.737 & 0.729 & 0.725 \\ 
%	Gauss\_5\_10\_10 & 0.878 & 0.829 & 0.795 & 0.704 & 0.873 & 0.875 & 0.873 & 0.872 & 0.875 & 0.868 \\ 
%	Gauss\_5\_50\_0 & 0.723 & 0.669 & 0.668 & 0.622 & 0.695 & 0.692 & 0.692 & 0.7 & 0.696 & 0.692 \\ 
%	Gauss\_5\_50\_5 & 0.646 & 0.596 & 0.621 & 0.57 & 0.622 & 0.619 & 0.627 & 0.631 & 0.625 & 0.626 \\
%	Gauss\_5\_50\_10 & 0.669 & 0.639 & 0.626 & 0.546 & 0.64 & 0.641 & 0.639 & 0.65 & 0.643 & 0.637 \\ 
%	Gauss\_7\_5\_0 & 1 & 1 & 0.852 & 0.773 & 0.999 & 1 & 0.999 & 1 & 0.999 & 0.998 \\ 
%	Gauss\_7\_5\_5 & 0.847 & 0.832 & 0.86 & 0.72 & 0.852 & 0.85 & 0.849 & 0.847 & 0.847 & 0.847 \\ 
%	Gauss\_7\_5\_10 & 0.979 & 0.963 & 0.839 & 0.744 & 0.981 & 0.979 & 0.98 & 0.979 & 0.978 & 0.978 \\ 
%	Gauss\_7\_10\_0 & 0.995 & 0.995 & 0.869 & 0.785 & 0.988 & 0.984 & 0.987 & 0.984 & 0.982 & 0.976 \\ 
%	Gauss\_7\_10\_5 & 0.988 & 0.994 & 0.853 & 0.732 & 0.988 & 0.994 & 0.99 & 0.994 & 0.993 & 0.994 \\ 
%	Gauss\_7\_10\_10 & 0.89 & 0.81 & 0.845 & 0.724 & 0.851 & 0.856 & 0.854 & 0.854 & 0.85 & 0.851 \\ 
%	Gauss\_7\_50\_0 & 0.867 & 0.834 & 0.784 & 0.693 & 0.832 & 0.835 & 0.84 & 0.833 & 0.835 & 0.846 \\ 
%	Gauss\_7\_50\_5 & 0.852 & 0.808 & 0.727 & 0.647 & 0.813 & 0.815 & 0.814 & 0.815 & 0.812 & 0.807 \\ 
%	Gauss\_7\_50\_10 & 0.829 & 0.761 & 0.742 & 0.645 & 0.807 & 0.799 & 0.807 & 0.804 & 0.804 & 0.799 \\ \hline
%\end{tabular}
%
%\end{center}
%
%\caption{AMI For Gaussian Datasets}
%\end{figure*}
%
%
%\begin{figure*}[h]
%\begin{center}
%\begin{tabular}{ | l | l | l | l | l | l | l | l | l | }
%\hline
%	Dataset & Approx & Approx & Approx & Approx & Approx & Approx & Approx & Approx \\ \hline
%	& k-medoids & Supermodular & PAM & 1-Swap+ & 2-Swap & 2-Swap+ & 3-Swap & 4-swaps   \\ \hline
%	Gauss\_3\_5\_0 & 1.159 & 1.481 & 1.013 & 1 & 1.011 & 1 & 1.013 & 1.009 \\ 
%	Gauss\_3\_5\_5 & 1.057 & 1.4 & 1.005 & 1.001 & 1.006 & 1.001 & 1.01 & 1.007 \\ 
%	Gauss\_3\_5\_10 & 1.047 & 1.377 & 1.007 & 1.004 & 1.014 & 1.007 & 1.009 & 1.008 \\ 
%	Gauss\_3\_10\_0 & 1.029 & 1.404 & 1.018 & 1.01 & 1.013 & 1.01 & 1.022 & 1.019 \\ 
%	Gauss\_3\_10\_5 & 1.063 & 1.329 & 1.025 & 1.013 & 1.021 & 1.012 & 1.02 & 1.026 \\ 
%	Gauss\_3\_10\_10 & 1.06 & 1.369 & 1.017 & 1.009 & 1.018 & 1.007 & 1.016 & 1.016 \\ 
%	Gauss\_3\_50\_0 & 1.112 & 1.354 & 1.05 & 1.026 & 1.037 & 1.025 & 1.049 & 1.044 \\ 
%	Gauss\_3\_50\_5 & 1.124 & 1.327 & 1.045 & 1.025 & 1.041 & 1.019 & 1.044 & 1.04 \\ 
%	Gauss\_3\_50\_10 & 1.121 & 1.316 & 1.044 & 1.023 & 1.037 & 1.021 & 1.043 & 1.043 \\ 
%	Gauss\_5\_5\_0 & 1.105 & 1.576 & 1.013 & 1 & 1.011 & 1 & 1.016 & 1.015 \\ 
%	Gauss\_5\_5\_5 & 1.07 & 1.436 & 1.03 & 1.002 & 1.028 & 1.005 & 1.026 & 1.024 \\ 
%	Gauss\_5\_5\_10 & 0.003 & 1.369 & 1.021 & 1.004 & 1.016 & 1.007 & 1.016 & 1.015 \\ 
%	Gauss\_5\_10\_0 & 1.148 & 1.619 & 1.029 & 1 & 1.026 & 1.001 & 1.028 & 1.029 \\ 
%	Gauss\_5\_10\_5 & 1.08 & 1.389 & 1.017 & 1.011 & 1.018 & 1.009 & 1.017 & 1.019 \\ 
%	Gauss\_5\_10\_10 & 1.152 & 1.458 & 1.022 & 1.006 & 1.023 & 1.007 & 1.021 & 1.026 \\
%	Gauss\_5\_50\_0 & 1.096 & 1.271 & 1.031 & 1.017 & 1.033 & 1.015 & 1.031 & 1.029 \\ 
%	Gauss\_5\_50\_5 & 1.089 & 1.272 & 1.03 & 1.02 & 1.027 & 1.014 & 1.033 & 1.032 \\ 
%	Gauss\_5\_50\_10 & 1.09 & 1.299 & 1.029 & 1.017 & 1.036 & 1.016 & 1.028 & 1.036 \\ 
%	Gauss\_7\_5\_0 & 1.214 & 1.59 & 1.016 & 1 & 1.018 & 1 & 1.014 & 1.016 \\ 
%	Gauss\_7\_5\_5 & 1.046 & 1.446 & 1.017 & 1.009 & 1.014 & 1.004 & 1.014 & 1.019 \\ 
%	Gauss\_7\_5\_10 & 1.153 & 1.53 & 1.01 & 1 & 1.007 & 1 & 1.01 & 1.009 \\ 
%	Gauss\_7\_10\_0 & 1.132 & 1.504 & 1.032 & 1 & 1.023 & 1 & 1.032 & 1.034 \\ 
%	Gauss\_7\_10\_5 & 1.159 & 1.569 & 1.031 & 1 & 1.028 & 1 & 1.027 & 1.024 \\ 
%	Gauss\_7\_10\_10 & 1.074 & 1.56 & 1.024 & 1.003 & 1.019 & 1.005 & 1.02 & 1.02 \\ 
%	Gauss\_7\_50\_0 & 1.104 & 1.321 & 1.029 & 1.01 & 1.031 & 1.011 & 1.027 & 1.026 \\ 
%	Gauss\_7\_50\_5 & 1.132 & 1.326 & 1.033 & 1.013 & 1.031 & 1.013 & 1.026 & 1.036 \\ 
%	Gauss\_7\_50\_10 & 1.107 & 1.345 & 1.027 & 1.014 & 1.025 & 1.012 & 1.031 & 1.026 \\ \hline
%\end{tabular}
%
%\end{center}
%
%\caption{Approximation Constants For Gaussian Datasets}
%\end{figure*}
%
%\subsection{P-Median}
%
%\begin{figure*}[h]
%\begin{center}
%\begin{tabular}{ | l | l | l | l | }
%\hline
%	Dataset & Optimal Objective & $k$ & $n$ \\ \hline
%	pmed1 & 5819 & 5 & 100 \\ 
%	pmed2 & 4093 & 10 & 100 \\ 
%	pmed3 & 4250 & 10 & 100 \\ 
%	pmed4 & 3034 & 20 & 100 \\
%	pmed5 & 1355 & 33 & 100 \\ 
%	pmed6 & 7824 & 5 & 200 \\ 
%	pmed7 & 5631 & 10 & 200 \\ 
%	pmed8 & 4445 & 20 & 200 \\ 
%	pmed9 & 2734 & 40 & 200 \\ 
%	pmed10 & 1255 & 67 & 200 \\ 
%	pmed11 & 7696 & 5 & 300 \\ 
%	pmed12 & 6634 & 10 & 300 \\ 
%	pmed13 & 4374 & 30 & 300 \\ 
%	pmed14 & 2968 & 60 & 300 \\ 
%	pmed15 & 1729 & 100 & 300 \\
%	pmed16 & 8162 & 5 & 400 \\ 
%	pmed17 & 6999 & 10 & 400 \\
%	pmed18 & 4809 & 40 & 400 \\ 
%	pmed19 & 2845 & 80 & 400 \\ 
%	pmed20 & 1789 & 133 & 400 \\ 
%	pmed21 & 9138 & 5 & 500 \\ 
%	pmed22 & 8579 & 10 & 500 \\ 
%	pmed23 & 4619 & 50 & 500 \\ 
%	pmed24 & 2961 & 100 & 500 \\ 
%	pmed25 & 1828 & 167 & 500 \\ 
%	pmed26 & 9917 & 5 & 600 \\ 
%	pmed27 & 8307 & 10 & 600 \\ 
%	pmed28 & 4498 & 60 & 600 \\ 
%	pmed29 & 3033 & 120 & 600 \\
%	pmed30 & 1989 & 200 & 600 \\ 
%	pmed31 & 10086 & 5 & 700 \\ 
%	pmed32 & 9297 & 10 & 700 \\ 
%	pmed33 & 4700 & 60 & 700 \\ 
%	pmed34 & 3013 & 140 & 700 \\ 
%	pmed35 & 10400 & 5 & 800 \\ 
%	pmed36 & 9934 & 10 & 800 \\ 
%	pmed37 & 5057 & 80 & 800 \\ 
%	pmed38 & 11060 & 5 & 900 \\ 
%	pmed39 & 9423 & 10 & 900 \\ 
%	pmed40 & 5128 & 90 & 900 \\ \hline
%\end{tabular}
%
%
%\end{center}
%
%\caption{Details for P-Median Datasets}
%\end{figure*}
%
%\begin{figure*}[h]
%\begin{center}
%\begin{tabular}{ | l | l | l | l | l | l | l | l | }
%\hline
%	Dataset & Time & Time & Time & Time & Time & Time & Time \\ \hline
%	&k-medoids & Supermodular & PAM & 1-Swap + & 2-Swap & 2-Swaps + & 3-Swap   \\ \hline
%	pmed1 & 0.0032 & 5 & 0.1 & 0.1 & 0.2 & 0.2 & 0.3 \\ 
%	pmed2 & 0.0069 & 6.4 & 0.2 & 0.2 & 0.3 & 0.3 & 0.4 \\ 
%	pmed3 & 0.0067 & 7.5 & 0.2 & 0.2 & 0.3 & 0.3 & 0.4 \\ 
%	pmed4 & 0.0125 & 8 & 0.3 & 0.4 & 0.5 & 0.6 & 0.6 \\ 
%	pmed5 & 0.0381 & 8.3 & 0.5 & 0.6 & 0.7 & 0.7 & 0.9 \\ 
%	pmed6 & 0.0065 & 19.3 & 0.3 & 0.4 & 0.5 & 0.6 & 0.8 \\ 
%	pmed7 & 0.0117 & 23.9 & 0.7 & 0.9 & 0.9 & 1.3 & 1.5 \\ 
%	pmed8 & 0.027 & 27.2 & 1.4 & 1.8 & 2.2 & 2.6 & 3.1 \\ 
%	pmed9 & 0.0642 & 29 & 3.4 & 3.5 & 3.3 & 4.8 & 4.7 \\ 
%	pmed10 & 0.7329 & 39.8 & 4.4 & 5.8 & 6.2 & 7.3 & 8.2 \\ 
%	pmed11 & 0.0124 & 42 & 0.6 & 0.8 & 1.4 & 1.2 & 2.2 \\ 
%	pmed12 & 0.0186 & 45.7 & 1.7 & 2 & 2.3 & 2.4 & 4.3 \\ 
%	pmed13 & 0.0685 & 69.1 & 6.6 & 5.3 & 7 & 8.4 & 11 \\ 
%	pmed14 & 0.2238 & 76.3 & 13.6 & 14.2 & 17.1 & 17.4 & 25 \\ 
%	pmed15 & 1.0969 & 100.2 & 18.4 & 19.7 & 23.1 & 28.7 & 35.5 \\ 
%	pmed16 & 0.0251 & 87.1 & 1.3 & 1.2 & 2.3 & 2.4 & 3.8 \\ 
%	pmed17 & 0.0581 & 97.1 & 3.5 & 2.5 & 3.9 & 4.8 & 6.8 \\ 
%	pmed18 & 0.2596 & 134.7 & 15.3 & 16.5 & 23.1 & 23.3 & 31.3 \\ 
%	pmed19 & 1.0193 & 153.5 & 34.9 & 34.1 & 40.3 & 47.9 & 57.8 \\ 
%	pmed20 & 1.7947 & 175 & 52.2 & 45 & 68.6 & 68.2 & 79.3 \\ 
%	pmed21 & 0.0327 & 109.4 & 1.8 & 2.1 & 4.1 & 4.2 & 6.1 \\ 
%	pmed22 & 0.0786 & 113.1 & 4.3 & 5.6 & 8.3 & 7.5 & 12.7 \\ 
%	pmed23 & 0.5123 & 199.8 & 35.9 & 34.8 & 46.9 & 52.3 & 62.2 \\ 
%	pmed24 & 1.7861 & 245.9 & 66.6 & 90.3 & 99.2 & 88.5 & 125.5 \\ 
%	pmed25 & 3.1433 & 451.9 & 103 & 118 & 126.8 & 146.4 & 138.1 \\ 
%	pmed26 & 0.0503 & 159.8 & 2.8 & 2.4 & 4.7 & 5.6 & 6.9 \\ 
%	pmed27 & 0.1265 & 183.4 & 7.1 & 6.1 & 11.7 & 10.7 & 13 \\ 
%	pmed28 & 0.8738 & 263.3 & 60.3 & 71.3 & 95.6 & 98.5 & 89.3 \\ 
%	pmed29 & 2.8418 & 424.3 & 152.6 & 146.8 & 196.5 & 172.8 & 167 \\ 
%	pmed30 & 5.2891 & 571 & 175 & 207.5 & 254.2 & 249.7 & 255.7 \\ 
%	pmed31 & 0.0728 & 225.6 & 4.9 & 4.4 & 8.2 & 6.6 & 9.2 \\ 
%	pmed32 & 0.1528 & 249.5 & 11 & 7.8 & 14.3 & 12.9 & 19 \\ 
%	pmed33 & 1.4723 & 440.1 & 112.8 & 91.9 & 143.9 & 158.5 & 154.1 \\ 
%	pmed34 & 3.6669 & 591.7 & 234.8 & 217.8 & 233.1 & 287.8 & 256.8 \\ 
%	pmed35 & 0.1133 & 298.3 & 4.8 & 5.9 & 7.5 & 9.3 & 11.9 \\ 
%	pmed36 & 0.2221 & 259.8 & 13 & 11.4 & 15.9 & 24 & 23.6 \\ 
%	pmed37 & 2.4409 & 571 & 190.5 & 149.7 & 172.8 & 282.3 & 224.7 \\ 
%	pmed38 & 0.1326 & 349.9 & 7.1 & 6.5 & 9.8 & 12.2 & 14.5 \\ 
%	pmed39 & 0.2676 & 419.8 & 16.5 & 13.6 & 22.6 & 25.5 & 29.4 \\ 
%	pmed40 & 3.5269 & 849.9 & 251.3 & 264.9 & 250.4 & 391.7 & 350.2 \\ \hline
%\end{tabular}
%
%
%\end{center}
%
%\caption{Running Time for P-Median Datasets}
%\end{figure*}
%
%\begin{figure*}[h]
%\begin{center}
%\begin{tabular}{ | l | l | l | l | l | l | l | l | }
%\hline
%	Dataset & Approx & Approx & Approx & Approx & Approx & Approx & Approx \\ \hline
%	  & k-medoids & Supermodular & PAM & 1-Swap + & 2-Swap & 2-Swaps+ & 3-Swap \\ \hline
%	pmed1 & 1.113 & 1.409 & 1.03 & 1.013 & 1.029 & 1.005 & 1.018 \\ 
%	pmed2 & 1.158 & 1.486 & 1.044 & 1.011 & 1.038 & 1.011 & 1.044 \\ 
%	pmed3 & 1.161 & 1.442 & 1.053 & 1.01 & 1.037 & 1.01 & 1.046 \\ 
%	pmed4 & 1.16 & 1.5 & 1.07 & 1.028 & 1.061 & 1.019 & 1.056 \\ 
%	pmed5 & 1.373 & 1.681 & 1.06 & 1.026 & 1.071 & 1.033 & 1.051 \\ 
%	pmed6 & 1.09 & 1.286 & 1.015 & 1.008 & 1.016 & 1.008 & 1.012 \\ 
%	pmed7 & 1.126 & 1.367 & 1.027 & 1.011 & 1.023 & 1.01 & 1.025 \\ 
%	pmed8 & 1.179 & 1.489 & 1.064 & 1.017 & 1.051 & 1.021 & 1.048 \\ 
%	pmed9 & 1.249 & 1.559 & 1.065 & 1.031 & 1.063 & 1.024 & 1.063 \\ 
%	pmed10 & 1.462 & 1.839 & 1.076 & 1.037 & 1.076 & 1.039 & 1.076 \\ 
%	pmed11 & 1.071 & 1.369 & 1.014 & 1.004 & 1.008 & 1.005 & 1.011 \\ 
%	pmed12 & 1.144 & 1.368 & 1.025 & 1.007 & 1.028 & 1.007 & 1.025 \\ 
%	pmed13 & 1.209 & 1.446 & 1.047 & 1.019 & 1.05 & 1.021 & 1.05 \\ 
%	pmed14 & 1.243 & 1.504 & 1.056 & 1.026 & 1.053 & 1.025 & 1.049 \\ 
%	pmed15 & 1.61 & 1.658 & 1.067 & 1.032 & 1.068 & 1.03 & 1.06 \\ 
%	pmed16 & 1.062 & 1.333 & 1.012 & 1.004 & 1.012 & 1.005 & 1.008 \\ 
%	pmed17 & 1.149 & 1.359 & 1.023 & 1.014 & 1.028 & 1.013 & 1.025 \\ 
%	pmed18 & 1.169 & 1.414 & 1.044 & 1.016 & 1.033 & 1.016 & 1.04 \\ 
%	pmed19 & 1.261 & 1.537 & 1.056 & 1.022 & 1.059 & 1.02 & 1.055 \\ 
%	pmed20 & 1.501 & 1.73 & 1.079 & 1.032 & 1.076 & 1.027 & 1.073 \\ 
%	pmed21 & 1.096 & 1.384 & 1.027 & 1.005 & 1.015 & 1.003 & 1.021 \\ 
%	pmed22 & 1.128 & 1.371 & 1.026 & 1.015 & 1.018 & 1.007 & 1.021 \\ 
%	pmed23 & 1.151 & 1.42 & 1.041 & 1.019 & 1.045 & 1.016 & 1.041 \\ 
%	pmed24 & 1.271 & 1.519 & 1.058 & 1.019 & 1.055 & 1.019 & 1.052 \\ 
%	pmed25 & 1.449 & 1.711 & 1.07 & 1.033 & 1.077 & 1.035 & 1.073 \\ 
%	pmed26 & 1.073 & 1.317 & 1.018 & 1.008 & 1.012 & 1.003 & 1.014 \\ 
%	pmed27 & 1.113 & 1.36 & 1.017 & 1.008 & 1.018 & 1.009 & 1.017 \\ 
%	pmed28 & 1.168 & 1.386 & 1.046 & 1.011 & 1.037 & 1.015 & 1.04 \\ 
%	pmed29 & 1.263 & 1.497 & 1.055 & 1.026 & 1.057 & 1.024 & 1.057 \\ 
%	pmed30 & 1.456 & 1.643 & 1.067 & 1.033 & 1.064 & 1.032 & 1.061 \\ 
%	pmed31 & 1.082 & 1.336 & 1.01 & 1.003 & 1.006 & 1.001 & 1.013 \\ 
%	pmed32 & 1.12 & 1.344 & 1.02 & 1.011 & 1.018 & 1.011 & 1.017 \\ 
%	pmed33 & 1.182 & 1.432 & 1.043 & 1.018 & 1.044 & 1.017 & 1.04 \\ 
%	pmed34 & 1.323 & 1.539 & 1.061 & 1.024 & 1.059 & 1.021 & 1.056 \\ 
%	pmed35 & 1.083 & 1.292 & 1.011 & 1.005 & 1.015 & 1.007 & 1.01 \\ 
%	pmed36 & 1.138 & 1.334 & 1.016 & 1.009 & 1.016 & 1.011 & 1.016 \\ 
%	pmed37 & 1.157 & 1.439 & 1.042 & 1.02 & 1.05 & 1.017 & 1.047 \\ 
%	pmed38 & 1.063 & 1.288 & 1.007 & 1.009 & 1.008 & 1.006 & 1.01 \\ 
%	pmed39 & 1.127 & 1.42 & 1.02 & 1.007 & 1.014 & 1.007 & 1.015 \\ 
%	pmed40 & 1.187 & 1.423 & 1.046 & 1.017 & 1.043 & 1.019 & 1.039 \\ \hline
%\end{tabular}
%
%
%\end{center}
%
%\caption{Approximation Constants for P-Median Datasets}
%\end{figure*}



\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
