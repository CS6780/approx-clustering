\documentclass[conference, 10pt, final]{IEEEtran}

\usepackage{cite}
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage[pdftex]{graphicx}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=footnotesize]{subfig}
\usepackage{stfloats}
\usepackage{url}


\begin{document} 

\title{Improved Hierarchical Clustering Through Nesting}


\author{\IEEEauthorblockN{Alice Paul, Calvin Wylie, and David Lingenbrink}
\IEEEauthorblockA{Operations Research and Information Engineering, Cornell University, Ithaca, NY.  \\
Email: \{ ajp336, cjw278, dal299 \} @cornell.edu}}


\maketitle

\begin{abstract}
Partitional clustering algorithms can organize large data into smaller, more informative groups. Hierarchical clustering, in addition, offers a more global view of the data structure by producing a hierarchy of clusters starting from individual data points and merging towards the cluster of all data. Typically, hierarchical clustering is done by greedily either merging or dividing clusters, called agglomerative and divisive clustering, respectively. Lin et al.~\cite{Lin} proposed a nesting framework that produces a hierarchy from partitional clusterings for every possible number of clusters. This algorithm was introduced as an approximation algorithm for the hierarchical $k$-median problem and guarantees the resulting hierarchical layers are not too far away from the original clusters. However, this algorithm has not been previously placed in a clustering context. We test this algorithm with various methods for producing the inputted partitional clusterings versus the traditional hierarchical clustering methods and show both the approximation ratio and FScore measure improve significantly. We also propose a heuristic for this nesting framework which improves the running time and only suffers a slight decrease in approximation and FScore.
\end{abstract}


\section{Introduction and Notation}
As the data we have access to grows in variety and size and becomes cheaper to store, we need methods to efficiently learn from this data. 
Partitioning data (clustering) into similar groups (clusters) is one of the key tools to learn about a set of unlabeled data such as a set of customers who have visited a website or a set of handwritten documents.
Clustering in these situations helps us learn underlying structure and classifications of our data, and it allows large data sets to be presented in a simplified and compressed view \cite{Jain}.

There are two main branches of clustering: partitional (or flat) and hierarchical. In partitional clustering, we often specify a number $k$ and pairwise distances between the data points, representing a measure of dissimilarity such as Euclidean distance. The clustering algorithm then partitions the data into $k$ disjoint sets where the sets represent relatively similar data. In hierarchical clustering, on the other hand, we are interested in finding a nested tree of partitions. This offers the user a global view of the data and allows the user to choose a ``natural'' level of clustering \cite{ESL}. For example, a retailer could use a hierarchical clustering of customers to first find regions of customers in order to tailor the products offered in each region. The retailer could then also offer specialized discounts to subgroups in each region based on each cluster's shopping records. 

Clustering algorithms date back more than 50 years, and there have been many variations introduced since \cite{Jain}. However, clustering algorithms are often formulated as NP-hard problems with non-convex objective functions, which are inherently hard to solve \cite{Jain}.
One common objective function for partitional clustering is to minimize the sum of distances from each point to the median point of its cluster, called $k$-median or $k$-medoids clustering. More specifically, suppose we are given a set $X$ and a distance (dissimilarity) metric $d(i,j)$ for all $i,j \in X$. The goal is to find a set of centers $S \subseteq X$ such that $|S| \leq k$ so as to minimize 
\[ f(S) = \sum_{ i\in X} \min_{j \in S} d(i,j). \]
The clustering is obtained from the set of centers $S$ by assigning each data point to its closest center.  An $\alpha$-approximation algorithm for this problem produces a set $S$ such that $f(S) \leq \alpha \cdot f(O)$ for all $O \subseteq X$ with $|O| = k$. 

[ADD FIGURES SHOWING BOTH TYPES OF CLUSTERING]

This objective function can then be extended to the hierarchical $k$-median problem which represents an objective function relating to the hierarchical clustering problem. Suppose we represent a solution to the $k$-median problem as a pair $(S, a)$ where $S \subseteq X$ such that $|S| \leq k$ and $a$ is an assignment function that maps each $i \in X$ to a center in $S$. Then, for any $i \in S$, $a^{-1}(i)$ is the cluster centered at $i$. We define $\mathrm{cost}(S,a) := \sum_{i \in X} d(i, a(i))$. This is the same objective function as above except the assignment is given rather than implied.

Given solution pairs $(S_1, a_1)$ and $(S_2, a_2)$, we say that $(S_1, a_1)$ is \emph{nested} in $(S_2, a_2)$ (denoted $(S_1, a_1) \subseteq (S_2, a_2)$) if the following conditions hold:
\begin{enumerate}[\IEEEsetlabelwidth{3)}]
	\item $S_1 \subset S_2$, 
	\item $\forall  j \in X$, if $a_2(j) \in S_1$, then $a_1(j) = a_2(j)$, 
	\item $\forall j, k \in X$, if $a_2(j) = a_2(k)$, then $a_1(j) = a_1(k)$.
\end{enumerate}
A solution to the hierarchical $k$-median problem is a nested chain of solutions and the hierarchical clustering given by looking at the clusters on each level. However, we want this solution to stay close to the optimal $k$-median solutions when looking at any level of the hierarchy. An $\alpha$-approximation algorithm for this problem ensures that for all $k = 1, \ldots, |X|$ there exists $(S_i, a_i)$ in our chain of solutions such that $|S_i| \leq k$ and $\mathrm{cost}(S_i, a_i) \leq \alpha \cdot f(O)$ for all $O \subseteq X$ with $|O| = k$.

While some flat clustering methods use the $k$-median objective function to inform their decisions and sometimes offer an approximation guarantee, popular methods for hierarchical clustering do not and rely on greedy algorithms which have no guarantee on the solution produced with respect to the objective function but are typically fast and easy to implement. This paper will look at how using this objective function can vastly improve the hierarchical clusterings produced. 
In particular, we will show how an approximation framework for this problem which takes in flat clusterings for all values of $k$ compares in practice to some of the most widely used clustering algorithms both in the quality of the solution and in computation time. The algorithms we will use to produce these flat clusterings will include both approximation algorithms for the $k$-median problem, which will offer an overall approximation guarantee for the hierarchical $k$-median problem, and some heuristic algorithms that have been shown to perform well in practice. 

\section{Literature Review}

Arguably the most common partitional clustering method is $k$-means, in which we want to choose $k$ cluster centers points in space to minimize the sum of squared distances from each point to its closest cluster center \cite{Jain}.  $k$-means is usually solved using Lloyd's algorithm (also referred to simply as the $k$-means algorithm), which iteratively greedily updates the centers of each cluster (assigning points to the closest center) until the solution becomes stable \cite{Jain}. Lloyd's algorithm is only guaranteed to reach a locally optimum solution and can be very sensitive to outliers and the initial points chosen \cite{Kanungo}.

The $k$-median objective function removes some worst-case behavior of $k$-means by restricting the cluster centers to be data points.  There are two common algorithms for this problem. The first is a local search algorithm called partition around medoids (PAM), which starts with $k$ randomly chosen data points as centers, and looks to swap one of the centers with another data point to improve the objective value. This repeats until no more improving swaps exist \cite{ESL}. The generalization of this algorithm which considers swaps of size $\leq p$ is a $(3 + \frac{2}{p})$-approximation algorithm \cite{Arya}.
Another common heuristic (which we shall call $k$-medoids throughout this paper), which is more computationally efficient than PAM \cite{Park}, is a variation on Lloyd's $k$-means algorithm.  Starting with randomly chosen centers, we assign each data point to its closest center, and then update the center within each cluster \cite{Park}. The algorithm repeats this until the solution becomes stable.

The $k$-median problem is also a fundamental problem in approximation algorithms where is often presented in the context of opening service centers or factories to serve clients. However, these algorithms are often not implemented for clustering despite their improved guarantees on the solution. The best known approximation algorithm for this problem has an approximation guarantee of 2.592 and is produced using linear programming \cite{Wu}. A simpler LP rounding algorithm by Charikar and Li has a slightly worse guarantee of 3.25 but is easier to implement in practice \cite{Charikar}. 

\subsection{Hierarchical Clustering}
The most common method for hierarchical clustering is agglomerative clustering. Agglomerative clustering starts with each data point in its own cluster, and proceeds to recursively merge the two closest clusters to form the next clustering level in the hierarchy, repeating until the entire data set is a single cluster.  There are several choices on how to measure distance in order to choose which clusters to merge. Single linkage (also called nearest neighbor) defines the distance between two clusters to be the minimum pairwise distance between the data points in each cluster.  Complete linkage clustering instead defines the distance to be the largest pairwise distance. A method between these two extremes is group average linkage which looks at the average pairwise distance between clusters. This strikes a balance between the closeness and compactness of the two techniques above and gives more reasonable results in practice \cite{ESL}. Lastly, there is Ward's method which chooses the two clusters to merge so as to minimize the sum of each cluster's variance \cite{Ward}.

Approximation algorithms for hierarchical clustering have also been studied, where we aim to identify a hierarchy of clusters so as to minimize the ratio of the objective value on every level in the hierarchy of size $k$ to the optimal objective value of the flat partitional clustering problem with $k$ clusters. Note that it may not always be possible to find a solution with approximation ratio 1. The hierarchical $k$-median problem was originally introduced by Plaxton who gave a 238.88 approximation algorithm \cite{Plaxton}. This was improved by Lin et al.~\cite{Lin} who gave an incremental approximation algorithm framework. In particular, their approach relies on a black box $k$-median solver and then uses these solutions for $k=1, \ldots, n$ to greedily construct a hierarchical clustering that increases the objective value by at most a factor of $20.71$ (or $10.03$ when randomized) at each level. Using the local $p$-swap $(3+\frac{2}{p})$-approximation algorithm mentioned above as the black-box $k$-median solver performed well in experimental tests \cite{Nagarajan}. While the authors tested the variants of this algorithm for different $k$-median approximation algorithms, it has not been tested against traditional clustering algorithms or on any labeled data sets. 

Several algorithms have been built upon agglomerative clustering that we choose not to test our proposed algorithms against. Divisive clustering, similar to agglomerative, starts with the data in one cluster and repeatedly splits a cluster on each level. This method has not been studied nearly as much as agglomerative clustering \cite{ESL} but has had relative success in classifying document datasets \cite{Zhao}.  Algorithms such as the CURE, ROCK, and Chameleon \cite{Guha_CURE, Guha_ROCK, Karypis} incorporate non-spherical cluster shapes and cluster density, but do not relate as much to the above approximation algorithm in objective. We also choose not compare the approximation framework with Bayesian hierarchical methods for the same reasons since these methods require many more parameters and add additional prior information that our algorithms do not use. 

\section{Proposed Algorithms}

Recall the hierarchical $k$-median problem: given a set $X$ and a distance metric $d(i,j)$ for all $i,j \in X$, we will represent a solution to the $k$-median problem as a pair $(S, a)$ where $S \subseteq X$ such that $|S| \leq k$ and $a$ is an assignment function that maps each $i \in X$ to a center in $S$.
 A solution to the hierarchical $k$-median problem is a chain of nested solutions $(S_1, a_1) \subseteq (S_2, a_2) \subseteq \ldots \subseteq (S_l, a_l)$. However, we want this solution to stay close to the optimal $k$-median solutions when looking at any level of the hierarchy.  
Consider any $k$-median algorithm $\mathcal{A}$. After running $\mathcal{A}$ for all potential values of $k$, Lin et al.~\cite{Lin} introduce an augmentation step that will nest these clusterings into the hierarchical framework and only increase costs by a bounded factor. 

First, set $i=1$, $S_0 = \emptyset$, $\beta = 3+\sqrt{3}$, and $\beta_0 = 1$ (for the randomized algorithm we switch $\beta = 6.355$ and set $\beta_0 = \beta^X$, where $X$ is chosen from a uniform distribution on $[0,1]$). Let $V_i$ be the solutions generated from running $\mathcal{A}$ for $k=n-i+1$ for $i=1, \ldots, n$. The cost of these solutions is determined by assigning each point to the nearest cluster center and is scaled so that the minimum cost for $k<n$ is at least one. We then order these solutions according to their cost into buckets of the form $[0,0], (\beta_0, \beta_0 \beta], (\beta_0 \beta, \beta_0 \beta^2], \ldots $. 

From each non-empty bucket, pick the solution with the smallest number of cluster centers, and let these solutions be $\overline{V}_1, \overline{V}_2, \ldots \overline{V}_r$. We then inductively build our nested solutions. Given $(S_{i-1}, a_{i-1})$ and $\overline{V}_i$, we define $S_i$ by iteratively adding the closest point $j \in S_{i-1}$ to $l$ for all $l \in \overline{V}_i$. Note that $|S_{i}| \leq |\overline{V}_i|$. 

We then define an assignment $a_i$. For each $j \in X$ such that $a_{i-1}(j) \in S_{i}$, we set $a_i(j) = a_{i-1}(j)$. Otherwise, we set $a_i(j)$ to be the facility $i \in S_i$ such that assigning all points in $a_{i-1}^{-1}(j)$ (i.e. all points in $j$'s cluster in $S_{i-1}$) would have minimum cost. Using this assignment, $(a_i, S_i)$ is nested in $(a_{i-1}, S_{i-1})$. 

We continue this nesting until $|S_i| = 1$, reaching the top level of the hierarchy. After forming this hierarchical clustering, for any $k \in 1, \ldots, n$ \cite{Lin} guarantee that the cost of $S_i$ is at most $20.71$ times the cost of $V_k$ where $|S_{i-1}| \geq n-k+1 \geq |S_{i}|$ (in the randomized case the expected cost of $S_i$ is at most $10.03$ times the cost of $V_k$).

\subsection{Flat Clustering Algorithms}
This nesting framework allows us to choose the flat clustering algorithm $\mathcal{A}$. As mentioned above, PAM and $k$-medoids are two potential algorithms. PAM has the added benefit of being a 5-approximation algorithm for the flat $k$-median problem which yields to an overall approximation ratio of $20.71 \cdot 5 = 103.55$ for the hierarchical $k$-median problem ($10.03 \cdot 5 = 50.15$ randomized). We also considered the size of the local swaps in PAM to improve the approximation ratio but found that the approximation ratio of constructed solutions for given values of $k$ did not significantly improve while the computation time increased greatly. 

We also consider two linear programming based algorithms, one of which has an approximation guarantee. We first present the linear programming formulation for the $k$-median problem. Suppose we have a set $X$ and we let $s_i \in \{0,1\}$ be the variable representing whether or not we want to make $i$ a center for all $i \in X$. We also create a variable $a_{i,j} \in \{0, 1\}$ which represents whether or not we assign $i \in X$ to the cluster with center $j \in X$. Given this set and assignment, the $k$-median objective function is 
\begin{equation}
\sum_{i \in X} \sum_{j \in X} a_{i,j} d(i,j), 
\end{equation}
which we want to minimize. However, we are restricted by the fact that we can choose at most $k$ centers, every point must be assigned to a single cluster center, and that we can only assign a point to a center that is actually in $S$. This is represented by the three equations:
\begin{eqnarray}
\sum_{i \in X} s_i   \leq k & ,\\
\sum_{j \in X} a_{i,j}  = 1 & \forall i \in X, \\
a_{i,j}   \leq s_i & \forall i, j \in X.
\end{eqnarray}
Therefore, solving the following integer program would directly find the optimal set of centers and assignment for the $k$-median problem.
\begin{eqnarray}
\text{Min } & \sum_{i \in X} \sum_{j \in X} a_{i,j} d(i,j) \\
\text{s.t. } & \sum_{i \in X} s_i   \leq k  \\
& \sum_{j \in X} a_{i,j}  = 1 \quad \forall i \in X \\
& a_{i,j}   \leq s_i \quad \forall i, j \in X \\
& s_i \in \{0,1\} \quad \forall i \in X \\
& a_{i,j} \in \{0,1\} \quad \forall i, j \in X
\end{eqnarray}
Relaxing the constraints so that $s_i \in [0,1]$ for all $i \in X$ and $a_{i,j} \in [0,1]$ for all $i,j \in X$ yields the linear programming (LP) relaxation for the $k$-median problem, which we can solve in polynomial time.

Interestingly, the solution to the linear programming is often integer valued [NEED CITATION]. Based on this fact, we proposed a flat clustering algorithm that solves the LP relaxation and constructs $S$ by taking the points in $X$ with the $k$ highest $s_i$ values. We refer to this algorithm as LP-greedy from now on.
Charikar and Li \cite{charikar} introduce a 3.25-approximation algorithm that uses this LP solution to smartly construct a set of centers. We refer to this algorithm as LP-Charikar. We choose to implement this LP algorithm rather than the LP-based approximation algorithm with best approximation ratio because the algorithm is simpler to implement (and will most likely have a lower running time). 

The algorithm works by first choosing a subset of data points $C \subseteq X$ to represent the demand. For each data point $i$, let $d_{av}(i) = \sum_{j \in X : a_{i,j} > 0} s_i \cdot d(i,j)$. We start with $C = \emptyset$. At each step, we choose a point $j \in X$ to add to $C$, and all points $j'$ such that $d(j,j') \leq 4 d_{av}(j') $ are removed from $X$. This is continued until $X$ is empty. 

$C$ now represents data points that are relatively far apart. These points then choose a bundle of points $U_j$ that represent potential centers they would like to be assigned to. In particular, for each point $j \in C$, we look at all points $i \in X$ such that $0 < d(i,j) < 1.5 R_j$, where $R_j$ is half the distance from $j$ to it's nearest point in $C$, and bundle these points into $U_j$ (if a point is in multiple bundles you choose one arbitrarily). Now these bundles of points will have a sum of $s_i$ values at least one half. Lastly, these bundles are matched together greedily by distance. For each $U_j$, we let $\mathrm{vol}(U_j) = \sum_{i \in U_j} s_i$. 

Now that the algorithm has grouped together points so the volume is at least 1, the algorithm starts to choosing points from these various groups through sampling. Here, opening $U_j$ refers to selecting a point $i \in U_j$ with probabilities relative to the $s_i$ values and adding that point to $S$. 
\begin{enumerate}[\IEEEsetlabelwidth{3)}]
\item For each matched $U_j$ and $U_{j'}$, the algorithm opens $U_j$ with probability $1-\mathrm{vol}(U_{j'})$, opens $U_{j'}$ with probability $1- \mathrm{vol}(U_j)$, and opens both $U_{j}$ and $U_{j'}$ with probability $\mathrm{vol}(U_j) + \mathrm{vol}(U_{j'}) -1$. 
\item For any unmatched $U_j$, it is opened with probability $\mathrm{vol}(U_j)$.
\item For any $i$ not in any $U_j$ such that $s_i > 0$, add $i$ to $S$ with probability $s_i$. 
\end{enumerate}
This sampling process returns a set of centers of size $k$  in expectation. For our purposes, we sample until we obtain a solution with exactly $k$ centers. 

\section{Implementation and Results}
We implemented the nesting hierarchical framework above in Python 2.7, offering options for both the randomized and deterministic versions. For the black-box $k$-median algorithm, we implemented $k$-medoids, PAM, LP-greedy, and LP-charikar in Python using Gurobi 6.0.3 to solve the linear programming instances. We compared the results to hierarchical agglomerative clustering with both average linkage and Ward's linkage (implemented in the scipy package in Python). 

To avoid cycling, we limited the number of iterations in $k$-medoids to 1000. For PAM, we also needed to limit the number of swaps to achieve a reasonable runtime. Instead of searching over all potential swaps, we sample at most $n$ potential ones. We also ensure that some minimum level progress is achieved by ensuring the algorithm improves by at least a factor of 0.9 on each iteration. At the end of PAM, we run 10 iterations of $k$-medoids to ensure the algorithm finds a local optimum. These choices were made by looking at the relative performance and computation time on random Gaussian data not tested below. For PAM and $k$-medoids, when solving for multiple values of $k$, we use the solution with $k$ centers as a warm start for $k+l$ centers by choosing an additional random $l$ centers. For the two LP algorithms, Gurobi uses a warm start from solving the LP for $k+l$ to solve it for $k$ because we found this reverse direction to be faster in practice. 

We also tested a more computationally friendly heuristic of the hierarchical framework which only buckets solutions for values of $k$ that are a power of two. This yields many fewer $k$-median instances to solve, reducing the run-time significantly. This was motivated by the fact that the difference between various $k$-median solutions will be larger when $k$ is small so we expect our sampling to still represent much of the underlying structure. 
 
For each data set and each clustering method, we report the approximation ratio, computation time, and FScore measure.  FScore is a measure that looks at the clusters represented on each level and how they represent the true clustering in terms of precision and recall. See \cite{Larsen} for more information. 
[ADD DEFINITION OF FSCORE AND MORE DETAIL]

[MAKE CODE AVAILABLE PUBLICLY]

Tests were conducted using Python 2.7.9 on a 64-bit Windows platform, on a 4 core 3.4 Ghz Intel i7 processor with 16 GB physical memory. Results are given in Tables 2-4  below with some results averaged due to space constraints. The results reported are average values across 10 trials.

The data sets tested against were:
\begin{LaTeXdescription}
\item[Gaussian] 27 synthetic $k$-center Gaussian data sets, generated in Python. In particular, Gauss\textunderscore d\textunderscore k\textunderscore e was generated by first choosing $k$ random centers in $[-1,1]^d$. Around each center, we then generated $(1000/k) \cdot \mathrm{Unif}[0,1]$ Gaussian points of which $e$ percent had standard deviation 0.25 (noisy) and $1-e$ percent had standard deviation $0.05$. 
\item[OR $p$-median Library] \cite{Beasley} From a collection of test data sets created by Beasley with 40 test sets of size $100, 200, \ldots, 900$ and varying values of $k$. 
\item[UCI Iris] \cite{Iris} Contains 4 attributes on 150 instances of flowers, each of with belongs to one of three species of iris.  One of the species is linearly separable from the others, but the other two are not.  One of the most used data sets in pattern recognition.
\item[UCI Soybean (Small)] \cite{Soybean} 47 instances, with 35 attributes data set of soybean disease from UCI.
\end{LaTeXdescription} 

\begin{table}[!t]
\caption{Some specifics about our datasets}
\label{table_specifics}
\centering
\begin{tabular}{ | l | l | l | l | l | }
\hline
 Dataset Name & Opt Obj & $k$ & $n$ & dim \\ \hline
Soybean & 114 & 4 & 47 & 35 \\ 
Iris & 98 & 3 & 150 & 4 \\ 
Gauss\_5\_5\_5 & 195.4 & 5 & 388 & 5 \\ 
Gauss\_5\_10\_5 & 270 & 10 & 544 & 5 \\ 
Gaussian avg & 241 & 22 & 507 & 5 \\ 
pmed5 & 1355 & 33 & 100 & \  \\ 
pmed20 & 1789 & 133 & 400 & \  \\ 
% pmed40 & 5128 & 90 & 900 & \  \\ 
pmedian avg & 5535 & 46 & 460 & \  \\ \hline
 \end{tabular}
\end{table}


\begin{table*}[!t]
\label{fig:hier_results_comp}
\caption{Average Computation Time (sec), FScore measure, and Approximation Coeffiecients for Hierarchical Clustering Algorithms. K-med and PAM represent the black-box $k$-median solvers used and rand and logn indicate whether the randomized and logarithmic setting were used, respectively.}
\centering
\begin{tabular}{ | l | l | l | l | l | l | l | }
\hline
	   & Time & Time & Time & Time & Time & Time \\ \hline
	Dataset & Ave Linkage & Ward Linkage & K-med & K-med logn ran & k-med ran & PAM logn ran \\ \hline
	Soybean & 0.0004 & 0.0002 & 0.286 & 0 & 0.294 & 0.167 \\ 
	Iris & 0.0021 & 0.0011 & 3.161 & 0.226 & 3.794 & 5.122 \\ 
	Gauss\_5\_5\_5 & 0.0097 & 0.0109 & 106.619 & 2.735 &120.893 & 147.137 \\
	Gauss\_5\_10\_5 & 0.0237 & 0.0256 & 383.854 & 10.259 & 383.298 & 494.554 \\
	%Gauss\_5\_50\_5 &  & & & & & \\
	Gaussian Average & 0.0229 & 0.0239 & 354.459 & 7.492 & 369.967 & 423.293 \\ 
	pmed5 &  0.0004 & N/A & 1.184 & 0.102 & 1.288 & 1.673   \\ 
	pmed25 & 0.0174 & N/A & 279.334 & 5.011 & 306.684 & 312.503   \\ 
	pmedian avg & 0.0067 & N/A &  87.355 & 1.957 & 94.548 & 101.576   \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[!t]
\label{fig:hier_results_FScore}
\caption{Average FScore measure for Hierarchical Clustering Algorithms.}
\centering
\begin{tabular}{ | l | l | l | l | l | l | l | }
\hline
	   & F-Score & F-Score & F-Score & F-Score & F-Score & F-Score \\ \hline
	Dataset & Ave Linkage & Ward Linkage & K-med & K-med logn rand & K-med rand & PAM logn rand \\ \hline
	Soybean & 0.615 & 0.554 & 0.772 & 0.643 & 0.742 & 0.633 \\ 
	Iris & 0.5 & 0.5 & 0.888 & 0.767 & 0.762 & 0.732 \\ 
	Gauss\_5\_5\_5 & 0.519 & 0.520 & 0.591 & 0.816 & 0.761 & 0.785 \\
	Gauss\_5\_10\_5 & 0.298 & 0.281 & 0.589 & 0.584 & 0.576 & 0.446 \\
	%Gauss\_5\_50\_5 &  & & & & & \\
	Gaussian Average & 0.393 & 0.378 & 0.649 & 0.616 & 0.624 & 0.624 \\ \hline
\end{tabular}
\end{table*}


\begin{table*}[!t]
\label{fig:hier_results_approx}
\caption{Average Approximation Coeffiecients for Hierarchical Clustering Algorithms.}
\centering
\begin{tabular}{ | l | l | l | l | l | }
\hline
	   & Approx & Approx & Approx & Approx \\ \hline
	Dataset & K-med & K-med logn rand & K-med rand & PAM logn rand \\ \hline
	Soybean & 1.585 & 2.334 & 1.711 & 2.001 \\ 
	Iris & 2560631.459 & 2550279.566 & 2787312.009 & 1623132.096 \\ 
	Gauss\_5\_5\_5 & 6.459 & 13.394 & 8.624 & 7.791 \\
	Gauss\_5\_10\_5 &  3.420 & 3.756 & 4.196 & 2.309  \\
	%Gauss\_5\_50\_5 &  & & &  \\
	Gaussian Average & 5.182 & 7.262  & 5.537 & 4.739 \\ 
	pmed5 & 11.367 & 51.141 & 16.261 & 11.950 \\ 
	pmed25 & 6.641 & 13.372 & 5.378 & 13.200 \\ 
	pmedian avg & 10.330 & 19.822 & 10.524 & 8.599 \\ \hline
\end{tabular}
\end{table*}

In general, the hierarchical framework performed significantly better than both methods of agglomerative clustering in terms of FScore measure, even for our logarithmic version. The FScore measures were very low for the agglomerative clustering methods, sometimes around 0.25, but were much closer to the AMI measures we saw in flat clustering for our implemented algorithms. 

For $k$-medoids in the non-logarithmic version and PAM in the logarithmic version, the hierarchical framework increased the computation time by three orders of magnitude, and increased by two orders of magnitude for $k$-medoids in the logarithmic version. Interestingly, the approximation factors did not change significantly between the randomized and deterministic versions for $k$-medoids. The approximation ratios were strongest as $n$ increased or as the dimension increased. In particular, for the Gaussian data in 7 dimensions, the approximation ratio was often around 3.  The high approximation ratio for the iris data set was due to the fact that the optimal solution for $k = n-1, n-2$ was very close to zero, and our heuristic $k$-median algorithm only sometimes picked up on these solutions. The approximation factor at lower levels was more consistent with the other results. 


\section{Conclusion and Future Work}
We implemented various simple approximation algorithms for partitional clustering, and a framework for hierarchical clustering.  We compared these algorithms, along with heuristic modifications, against the most common clustering algorithms used in practice.

For partitional clustering, we found that the approximation algorithms in the literature are in general slower and cannot improve much on current heuristics used despite their approximation guarantees. This was not true in the hierarchical clustering case where we saw significant improvement in FScore measures for our implemented methods and overall good approximation ratios where none were had before. In particular, our logarithmic nesting algorithm seems to provide the best balance between performance and computational efficiency.

In the future, we hope to explore how linear programming based methods perform in both the flat and hierarchical context as well and to implement our own agglomerative clustering to help make smarter local merges in the nesting framework.

\section*{Acknowledgements}
We would like to acknowledge David P. Williamson and Karthik Sridharan for their valuable feedback.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite


\bibliographystyle{IEEEtran}
\bibliography{submissionBib}





\end{document} 


